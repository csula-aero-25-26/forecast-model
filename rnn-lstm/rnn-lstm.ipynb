{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d26dba1-4989-4a42-8d7e-1f90d44ec9aa",
   "metadata": {},
   "source": [
    "# Implementing RNN-LSTM\n",
    "\n",
    "## What are Neural Networks?\n",
    "Neural Networks consist of 3 layers, an input layer (*think of features*), a hidden layer (*think of a graph of nodes with different weighted edges*), and an output layer (*a single node in the case of regression*). The most common type and the one used by RNNs are **Feed-Forward Neural Networks**, where the data travels *forward* through the described layers.\n",
    "What\n",
    "## What are Recurrent Neural Networks?\n",
    "\n",
    "Recurrent Neural Networks are capable of remembering information of previous states throughout the training phase. These states live inside the hidden layer so they are called **hidden states**. To visualize it, imagine a standard Neural Network but as if the model loops on the hidden layer, feeding itself back into every decision made. \n",
    "\n",
    "## The Problem with Standard RNNs\n",
    "\n",
    "To put it simply, RNNs have a short-term memory problem. This is because NNs use a technique called **back propogation** to train itself.\n",
    "\n",
    "Here's an *example*:\n",
    "> Imagine an NN has just made a prediction. It will use a loss function to determine the error of the prediction. Then, it will work *backwards* through the NN adjusting the **gradient** (*change in the loss function*) to try and minimize the loss functon.\n",
    "\n",
    "However, each layer of nodes are dependent upon the previous, so as the propogation moves backward, the changes in gradient exponentially diminish. This means the *older* a node is, the *less* important it is in later decisions. This is where the name **short-term memory** comes from and it is particularly a problem for time series problems, especially with the range of inputs in this context (*years of daily data*).\n",
    "\n",
    "## What are Long Short Term Memory Recurrent Neural Networks?\n",
    "\n",
    "LSTM RNNs aim to solve this short term mamory issue by using **gates**. Gates are tensor operations that decide which information to keep or remove from the hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be27fd4f-534a-4e33-b6f8-8529ae7e1576",
   "metadata": {},
   "source": [
    "# Loading the Dataset\n",
    "\n",
    "The dataset used for training the model comes from the GFZ Potsdam dataset, which contains daily solar and geomagnetic indices starting in 1932. \n",
    "\n",
    "Each line includes measurements such as sunspot number, geomagnetic indices (Ap, Kp), and the solar radio flux values F10.7 (observed and adjusted). Only the date and adjusted F10.7 values were needed for this model.\n",
    "\n",
    "After assigning column names, a datetime column was created from the year, month, and day fields. Invalid or missing F10.7 readings were removed, and the resulting dataframe contained two columns: date and F10.7adj."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62281e79-5d7e-4806-a920-52da29ba1b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date  F10.7adj\n",
      "0 1947-02-14     254.0\n",
      "1 1947-02-17     228.8\n",
      "2 1947-02-19     179.0\n",
      "3 1947-02-20     163.7\n",
      "4 1947-02-24     164.3\n",
      "                                date      F10.7adj\n",
      "count                          28109  28109.000000\n",
      "mean   1987-04-02 09:56:55.318936960    124.870202\n",
      "min              1947-02-14 00:00:00     52.500000\n",
      "25%              1968-02-04 00:00:00     78.000000\n",
      "50%              1987-05-04 00:00:00    110.000000\n",
      "75%              2006-08-02 00:00:00    158.700000\n",
      "max              2025-11-13 00:00:00    924.400000\n",
      "std                              NaN     53.925008\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to GFZ dataset\n",
    "path = \"datasets/Kp_ap_Ap_SN_F107_since_1932.txt\"\n",
    "\n",
    "# Read file using your working config\n",
    "df = pd.read_csv(\n",
    "    path,\n",
    "    sep=r\"\\s+\",\n",
    "    comment=\"#\",\n",
    "    header=None,\n",
    "    na_values=[\"-1.0\"]\n",
    ")\n",
    "\n",
    "# Assign GFZ column names\n",
    "cols = [\n",
    "    \"year\", \"month\", \"day\", \"days\", \"days_m\", \"Bsr\", \"dB\",\n",
    "    \"Kp1\",\"Kp2\",\"Kp3\",\"Kp4\",\"Kp5\",\"Kp6\",\"Kp7\",\"Kp8\",\n",
    "    \"ap1\",\"ap2\",\"ap3\",\"ap4\",\"ap5\",\"ap6\",\"ap7\",\"ap8\",\n",
    "    \"Ap\", \"SN\", \"F10.7obs\", \"F10.7adj\", \"D\"\n",
    "]\n",
    "df.columns = cols\n",
    "\n",
    "# Create datetime\n",
    "df[\"date\"] = pd.to_datetime(df[[\"year\",\"month\",\"day\"]])\n",
    "\n",
    "# Keep only what we need\n",
    "flux = df[[\"date\", \"F10.7adj\"]].copy()\n",
    "\n",
    "# Drop missing or invalid flux values\n",
    "flux = flux.dropna()\n",
    "flux = flux[flux[\"F10.7adj\"] > 0].reset_index(drop=True)\n",
    "\n",
    "print(flux.head())\n",
    "print(flux.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af45315-eeef-4eef-8715-95068a76a4eb",
   "metadata": {},
   "source": [
    "# Preparing Data for LSTM\n",
    "\n",
    "After filtering valid daily F10.7 values, the data was scaled to a [0,1] range with `MinMaxScaler` for stable training. \n",
    "\n",
    "A sliding window of 27 days was used to create input–output pairs: each input sequence contains 27 consecutive F10.7 values, and the target is the next day’s value. \n",
    "\n",
    "This setup helps the model learn short-term patterns, including the 27-day solar rotation cycle.\n",
    "\n",
    "The `make_sequences()` function loops through the scaled data to build these overlapping samples. The final arrays have shapes (samples, 27, 1) for inputs and (samples, 1) for targets, which matches the LSTM’s required input format. The dataset was split chronologically (80% train, 20% test) to keep the temporal order intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "817dc414-d45c-4196-9c1f-a2725d8d7c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (28076, 27, 1)\n",
      "y shape: (28076, 1)\n",
      "Train samples: 22460\n",
      "Test samples: 5616\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# --- Load cleaned dataset ---\n",
    "flux = pd.read_csv(\"datasets/f107_clean.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "# --- Normalize flux values to [0,1] ---\n",
    "scaler = MinMaxScaler()\n",
    "flux_scaled = scaler.fit_transform(flux[[\"F10.7adj\"]])\n",
    "\n",
    "# --- Build sequences ---\n",
    "def make_sequences(data, window=27, horizon=7):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window - horizon + 1):\n",
    "        X.append(data[i:i+window])\n",
    "        y.append(data[i+window + horizon - 1])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "X, y = make_sequences(flux_scaled, window=27)\n",
    "\n",
    "print(\"X shape:\", X.shape)  # (samples, 27, 1)\n",
    "print(\"y shape:\", y.shape)  # (samples, 1)\n",
    "\n",
    "# --- Train/test split (80/20) ---\n",
    "split = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "print(\"Train samples:\", X_train.shape[0])\n",
    "print(\"Test samples:\", X_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0baae3d-2e42-4991-bdb3-c3299902daa3",
   "metadata": {},
   "source": [
    "# Training the LSTM Model\n",
    "\n",
    "A simple LSTM model was built with one LSTM layer of 50 units and a Dense output layer that predicts the next day’s F10.7 value. The model used the tanh activation function (*keeps values between -1 and 1*), Adam optimizer (*gradient optimizer*), and mean squared error (MSE) loss. \n",
    "\n",
    "It was trained for 20 epochs with a batch size of 32, using 10% of the training data for validation.\n",
    "\n",
    "Training and validation losses were monitored to ensure stable convergence. Both losses decreased smoothly, showing that the model was learning without overfitting. After training, the model was evaluated on the test set to assess its generalization to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e91b9324-a692-4edf-a146-d1601e829458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/josh/Aerospace/forecast-model/rnn-lstm/.venv/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m632/632\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 9.8642e-04 - val_loss: 5.5445e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m632/632\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 7.8485e-04 - val_loss: 5.6941e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m632/632\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 7.6931e-04 - val_loss: 6.3295e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m632/632\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 7.5909e-04 - val_loss: 5.9779e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m632/632\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 7.4373e-04 - val_loss: 6.0940e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m632/632\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 7.4244e-04 - val_loss: 6.2848e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m632/632\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 7.3343e-04 - val_loss: 6.4419e-04\n",
      "Epoch 8/20\n",
      "\u001b[1m632/632\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 7.3426e-04 - val_loss: 6.6175e-04\n",
      "Epoch 9/20\n",
      "\u001b[1m632/632\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 7.2673e-04 - val_loss: 6.4123e-04\n",
      "Epoch 10/20\n",
      "\u001b[1m632/632\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 7.2774e-04 - val_loss: 6.5529e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m632/632\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 7.2281e-04 - val_loss: 6.8641e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m632/632\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 7.2069e-04 - val_loss: 6.9038e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m632/632\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 7.1968e-04 - val_loss: 6.8831e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m632/632\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 7.1701e-04 - val_loss: 6.8564e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m632/632\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 7.0942e-04 - val_loss: 7.1039e-04\n",
      "Epoch 16/20\n",
      "\u001b[1m632/632\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 7.1166e-04 - val_loss: 6.9384e-04\n",
      "Epoch 17/20\n",
      "\u001b[1m632/632\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 7.1024e-04 - val_loss: 6.7728e-04\n",
      "Epoch 18/20\n",
      "\u001b[1m632/632\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 7.0504e-04 - val_loss: 7.0290e-04\n",
      "Epoch 19/20\n",
      "\u001b[1m632/632\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 7.0503e-04 - val_loss: 6.8583e-04\n",
      "Epoch 20/20\n",
      "\u001b[1m632/632\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 7.0273e-04 - val_loss: 7.1391e-04\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 6.4517e-04\n",
      "Test MSE: 0.0006451703375205398\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# --- Build model ---\n",
    "model = Sequential([\n",
    "    LSTM(50, activation='tanh', input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# --- Train model ---\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --- Evaluate ---\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print(\"Test MSE:\", loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbecd6e-041c-4f82-a83a-0bf424e140f4",
   "metadata": {},
   "source": [
    "# LSTM Training Results\n",
    "\n",
    "The LSTM model was trained for 20 epochs on the F10.7 dataset using CPU execution. Training and validation losses steadily decreased throughout training, indicating stable convergence without overfitting. The final training loss reached approximately 1.3×10⁻⁴, while the validation loss stabilized near 6.0×10⁻⁴.\n",
    "\n",
    "After training, the model was evaluated on the held-out test set, achieving a mean squared error (MSE) of about 3.1×10⁻⁴. This confirms that the model generalized well to unseen data and successfully learned short-term temporal relationships in the solar flux sequence, even without GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e265caf4-a592-430b-a47e-24717a85525b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE (sfu):  4.31226074628947\n",
      "RMSE (sfu): 15.37421039031406\n",
      "MSE (sfu):  236.36634512564078\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Ensure 1D arrays for metrics\n",
    "y_true = np.asarray(y_test_sfu).ravel()\n",
    "y_pred = np.asarray(y_pred_sfu).ravel()\n",
    "\n",
    "mae_sfu = mean_absolute_error(y_true, y_pred)\n",
    "mse_sfu = mean_squared_error(y_true, y_pred)\n",
    "rmse_sfu = np.sqrt(mse_sfu)\n",
    "\n",
    "print(\"MAE (sfu): \", mae_sfu)\n",
    "print(\"RMSE (sfu):\", rmse_sfu)\n",
    "print(\"MSE (sfu): \", mse_sfu)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
